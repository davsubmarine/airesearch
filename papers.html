<!doctype html>
<html class="">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
		<meta name="description" content="Your daily dose of AI research from AK" />
		<meta property="fb:app_id" content="1321688464574422" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@huggingface" />
		<meta name="twitter:image" content="https://huggingface.co/front/thumbnails/v2-2.png" />
		<meta property="og:title" content="Daily Papers - Hugging Face" />
		<meta property="og:type" content="website" />
		<meta property="og:url" content="https://huggingface.co/papers/date/2024-07-15" />
		<meta property="og:image" content="https://huggingface.co/front/thumbnails/v2-2.png" />

		<link rel="stylesheet" href="/front/build/kube-a3d300f/style.css" />

		<link rel="preconnect" href="https://fonts.gstatic.com" />
		<link
			href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap"
			rel="stylesheet"
		/>

		<link
			rel="preload"
			href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css"
			as="style"
			onload="this.onload=null;this.rel='stylesheet'"
		/>
		<noscript>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" />
		</noscript>

		<script>const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>  

		<title>Daily Papers - Hugging Face</title>

		<script
			defer
			data-domain="huggingface.co"
			event-loggedIn="false"
			src="/js/script.pageview-props.js"
		></script>
		<script>
			window.plausible =
				window.plausible ||
				function () {
					(window.plausible.q = window.plausible.q || []).push(arguments);
				};
		</script>
		<script>
			window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","captchaDisabledOnSignup":true,"datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}};
		</script>
		<script type="text/javascript" src="https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js" defer></script>
	</head>
	<body class="flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DailyPapersPage">
		<div class="flex min-h-dvh flex-col"><div class="SVELTE_HYDRATER contents" data-target="SystemThemeMonitor" data-props="{&quot;isLoggedIn&quot;:false}"></div>

	<div class="SVELTE_HYDRATER contents" data-target="MainHeader" data-props="{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:false,&quot;isPro&quot;:false}"><header class="border-b border-gray-100 "><div class="w-full px-4 container flex h-16 items-center"><div class="flex flex-1 items-center"><a class="mr-5 flex flex-none items-center lg:mr-6" href="/"><img alt="Hugging Face's logo" class="w-7 md:mr-2" src="/front/assets/huggingface_logo-noborder.svg">
				<span class="hidden whitespace-nowrap text-lg font-bold md:block">Hugging Face</span></a>
			<div class="relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6"><input autocomplete="off" class="w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl " name="" placeholder="Search models, datasets, users..."   spellcheck="false" type="text" value="">
	<svg class="absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z" fill="currentColor"></path></svg>
	</div>
			<div class="flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden"><button class="relative z-40 flex h-6 w-8 items-center justify-center" type="button"><svg width="1em" height="1em" viewBox="0 0 10 10" class="text-xl" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" preserveAspectRatio="xMidYMid meet" fill="currentColor"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z"></path></svg>
		</button>

	</div></div>
		<nav aria-label="Main" class="ml-auto hidden lg:block"><ul class="flex items-center space-x-1.5 2xl:space-x-2"><li class="hover:text-indigo-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/models"><svg class="mr-1.5 text-gray-400 group-hover:text-indigo-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path class="uim-quaternary" d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path class="uim-tertiary" d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path class="uim-primary" d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
					Models</a>
			</li><li class="hover:text-red-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/datasets"><svg class="mr-1.5 text-gray-400 group-hover:text-red-500" style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
					Datasets</a>
			</li><li class="hover:text-blue-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/spaces"><svg class="mr-1.5 text-gray-400 group-hover:text-blue-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 25 25"><path opacity=".5" d="M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z" fill="currentColor"></path><path opacity=".75" fill-rule="evenodd" clip-rule="evenodd" d="M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z" fill="currentColor"></path><path opacity=".25" d="M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z" fill="currentColor"></path></svg>
					Spaces</a>
			</li><li class="hover:text-yellow-700 max-xl:hidden"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/posts"><svg class="mr-1.5 text-gray-400 group-hover:text-yellow-500 text-yellow-500!" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 12 12" preserveAspectRatio="xMidYMid meet"><path fill="currentColor" fill-rule="evenodd" d="M3.73 2.4A4.25 4.25 0 1 1 6 10.26H2.17l-.13-.02a.43.43 0 0 1-.3-.43l.01-.06a.43.43 0 0 1 .12-.22l.84-.84A4.26 4.26 0 0 1 3.73 2.4Z" clip-rule="evenodd"></path></svg>
					Posts</a>
			</li><li class="hover:text-yellow-700"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/docs"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="mr-1.5 text-gray-400 group-hover:text-yellow-500" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 16 16"><path d="m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z" fill="currentColor" class="dark:opacity-40"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z" fill="currentColor" class="opacity-40 dark:opacity-100"></path><path d="M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z" fill="currentColor" class="dark:opacity-40"></path></svg>
					Docs</a>
			</li><li class="hover:text-black dark:hover:text-white"><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/enterprise"><svg class="mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white" xmlns="http://www.w3.org/2000/svg" fill="none" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 33 27"><path fill="currentColor" fill-rule="evenodd" d="M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z" clip-rule="evenodd"></path></svg>
					Enterprise</a>
			</li>

		<li><a class="group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100" href="/pricing">Pricing
			</a></li>

		<li><div class="relative group">
	<button class="px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center " type="button">
		<svg class=" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 18" preserveAspectRatio="xMidYMid meet"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z" fill="currentColor"></path></svg>
			
		</button>
	
	
	</div></li>
		<li><hr class="h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800"></li>
		<li><a class="block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100" href="/login">Log In
				</a></li>
			<li><a class="whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black" href="/join">Sign Up
					</a></li></ul></nav></div></header></div>
	
	
	
	<div class="SVELTE_HYDRATER contents" data-target="SSOBanner" data-props="{}"></div>
	
	

	<main class="flex flex-1 flex-col"><div class="SVELTE_HYDRATER contents" data-target="DailyPapersBannerSubscribe" data-props="{&quot;isLoggedIn&quot;:false}"><div class="-mt-px flex h-9 w-full justify-center text-gray-600"><svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z"></path></svg>
	<div class="flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1"><div class="rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600">new</div>
			<p class="hidden sm:inline">Get trending papers in your email inbox once a day!</p>
			<p class="inline sm:hidden">Get trending papers in your email inbox!</p>
			<a href="/login?next=%2Fpapers" class="btn px-2! text-sm leading-none">Subscribe</a></div>
	<svg class="hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 110 41"><path fill="currentColor" d="M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z"></path></svg></div></div>
	<div class="SVELTE_HYDRATER contents" data-target="DailyPapers" data-props="{&quot;canSubmit&quot;:false,&quot;dateString&quot;:&quot;2024-07-15&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09025&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;669488068db712bad712e469&quot;,&quot;name&quot;:&quot;Yuzhang Tian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46a&quot;,&quot;name&quot;:&quot;Jianbo Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46b&quot;,&quot;name&quot;:&quot;Haoyu Dong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46c&quot;,&quot;name&quot;:&quot;Junyu Xiong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46d&quot;,&quot;name&quot;:&quot;Shiyu Xia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46e&quot;,&quot;name&quot;:&quot;Mengyu Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e46f&quot;,&quot;name&quot;:&quot;Yun Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e470&quot;,&quot;name&quot;:&quot;José Cambronero&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e471&quot;,&quot;name&quot;:&quot;Yeye He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e472&quot;,&quot;name&quot;:&quot;Shi Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488068db712bad712e473&quot;,&quot;name&quot;:&quot;Dongmei Zhang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T06:34:21.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T00:53:26.334Z&quot;,&quot;title&quot;:&quot;SpreadsheetLLM: Encoding Spreadsheets for Large Language Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Spreadsheets, with their extensive two-dimensional grids, various layouts,\nand diverse formatting options, present notable challenges for large language\nmodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering an\nefficient encoding method designed to unleash and optimize LLMs' powerful\nunderstanding and reasoning capability on spreadsheets. Initially, we propose a\nvanilla serialization approach that incorporates cell addresses, values, and\nformats. However, this approach was limited by LLMs' token constraints, making\nit impractical for most applications. To tackle this challenge, we develop\nSheetCompressor, an innovative encoding framework that compresses spreadsheets\neffectively for LLMs. It comprises three modules: structural-anchor-based\ncompression, inverse index translation, and data-format-aware aggregation. It\nsignificantly improves performance in spreadsheet table detection task,\noutperforming the vanilla approach by 25.6% in GPT4's in-context learning\nsetting. Moreover, fine-tuned LLM with SheetCompressor has an average\ncompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,\nsurpassing the best existing models by 12.3%. Finally, we propose Chain of\nSpreadsheet for downstream tasks of spreadsheet understanding and validate in a\nnew and demanding spreadsheet QA task. We methodically leverage the inherent\nlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM is\nhighly effective across a variety of spreadsheet tasks.&quot;,&quot;upvotes&quot;:135,&quot;discussionId&quot;:&quot;669488088db712bad712e4c1&quot;,&quot;ai_keywords&quot;:[&quot;SpreadsheetLLM&quot;,&quot;vanilla serialization&quot;,&quot;SheetCompressor&quot;,&quot;structural-anchor-based compression&quot;,&quot;inverse index translation&quot;,&quot;data-format-aware aggregation&quot;,&quot;spreadsheet table detection&quot;,&quot;Chain of Spreadsheet&quot;,&quot;spreadsheet QA task&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T02:34:21.000Z&quot;,&quot;title&quot;:&quot;SpreadsheetLLM: Encoding Spreadsheets for Large Language Models&quot;,&quot;summary&quot;:&quot;Spreadsheets, with their extensive two-dimensional grids, various layouts,\nand diverse formatting options, present notable challenges for large language\nmodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering an\nefficient encoding method designed to unleash and optimize LLMs' powerful\nunderstanding and reasoning capability on spreadsheets. Initially, we propose a\nvanilla serialization approach that incorporates cell addresses, values, and\nformats. However, this approach was limited by LLMs' token constraints, making\nit impractical for most applications. To tackle this challenge, we develop\nSheetCompressor, an innovative encoding framework that compresses spreadsheets\neffectively for LLMs. It comprises three modules: structural-anchor-based\ncompression, inverse index translation, and data-format-aware aggregation. It\nsignificantly improves performance in spreadsheet table detection task,\noutperforming the vanilla approach by 25.6% in GPT4's in-context learning\nsetting. Moreover, fine-tuned LLM with SheetCompressor has an average\ncompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,\nsurpassing the best existing models by 12.3%. Finally, we propose Chain of\nSpreadsheet for downstream tasks of spreadsheet understanding and validate in a\nnew and demanding spreadsheet QA task. We methodically leverage the inherent\nlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM is\nhighly effective across a variety of spreadsheet tasks.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09025.png&quot;,&quot;numComments&quot;:18,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09450&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;66948c1b3cd89359e4452143&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;671880ff5f1f9948bf2ce2d1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qOQW6D05xIM1HPc-S_adk.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zafeirios Fountas&quot;,&quot;user&quot;:&quot;zfountas&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zafeirios Fountas&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-10-23T07:35:21.165Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452144&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;66840bc08f6c78ebd41f34ea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/85dcb534ff080ee1ca52d82eb41e1fc4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Martin Antoine Benfeghoul&quot;,&quot;user&quot;:&quot;m84366023&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Martin A Benfeghoul&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-16T20:16:25.146Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452145&quot;,&quot;name&quot;:&quot;Adnan Oomerjee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452146&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63eb8bffd64e6436e22e68ea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/29aeb53c878c047a6c762d7449a50bf1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fenia Christopoulou&quot;,&quot;user&quot;:&quot;fenchri&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Fenia Christopoulou&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T13:34:42.398Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452147&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6480af33c0d3c70316d2a333&quot;,&quot;avatarUrl&quot;:&quot;/avatars/95ec51e5408888d33b07696301ad34b6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gerasimos (Makis) Lampouras&quot;,&quot;user&quot;:&quot;glampouras&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Gerasimos Lampouras&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-31T12:49:37.253Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452148&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;631c375768f7da9ad2496bf6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haitham Bou Ammar&quot;,&quot;user&quot;:&quot;hba123&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Haitham Bou-Ammar&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-09-23T13:07:15.120Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948c1b3cd89359e4452149&quot;,&quot;name&quot;:&quot;Jun Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T17:34:03.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T01:10:47.165Z&quot;,&quot;title&quot;:&quot;Human-like Episodic Memory for Infinite Context LLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.&quot;,&quot;upvotes&quot;:62,&quot;discussionId&quot;:&quot;66948c1c3cd89359e445218e&quot;,&quot;ai_keywords&quot;:[&quot;Bayesian surprise&quot;,&quot;graph-theoretic boundary refinement&quot;,&quot;episodic events&quot;,&quot;similarity-based retrieval&quot;,&quot;temporally contiguous retrieval&quot;,&quot;event segmentation&quot;,&quot;EM-LLM&quot;,&quot;LongBench dataset&quot;,&quot;InfLLM model&quot;,&quot;PassageRetrieval task&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T13:34:03.000Z&quot;,&quot;title&quot;:&quot;Human-like Episodic Memory for Infinite Context LLMs&quot;,&quot;summary&quot;:&quot;Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09450.png&quot;,&quot;numComments&quot;:6,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.07874&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcef&quot;,&quot;name&quot;:&quot;Ben Cohen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf0&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;645e9d6d9c8e15af60a7d44f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645e9d6d9c8e15af60a7d44f/uCuZRH2YcYktidW-Re9Xp.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Emaad Khwaja&quot;,&quot;user&quot;:&quot;Emaad&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Emaad Khwaja&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-16T20:16:20.146Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf1&quot;,&quot;name&quot;:&quot;Kan Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf2&quot;,&quot;name&quot;:&quot;Charles Masson&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf3&quot;,&quot;name&quot;:&quot;Elise Ramé&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf4&quot;,&quot;name&quot;:&quot;Youssef Doubli&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669533cff5481f2dcd00dcf5&quot;,&quot;name&quot;:&quot;Othmane Abou-Amal&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/645e9d6d9c8e15af60a7d44f/7XPYOicouddIn7uG2CoBR.png&quot;],&quot;publishedAt&quot;:&quot;2024-07-10T17:40:30.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T13:09:27.980Z&quot;,&quot;title&quot;:&quot;Toto: Time Series Optimized Transformer for Observability&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;645e9d6d9c8e15af60a7d44f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645e9d6d9c8e15af60a7d44f/uCuZRH2YcYktidW-Re9Xp.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Emaad Khwaja&quot;,&quot;user&quot;:&quot;Emaad&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.&quot;,&quot;upvotes&quot;:32,&quot;discussionId&quot;:&quot;669533d1f5481f2dcd00dd4b&quot;,&quot;ai_keywords&quot;:[&quot;Time Series Optimized Transformer&quot;,&quot;Toto&quot;,&quot;time series forecasting&quot;,&quot;foundation model&quot;,&quot;generalized time series benchmarks&quot;,&quot;one trillion time series data points&quot;,&quot;numerical metric data points&quot;,&quot;observability data&quot;,&quot;few-shot performance&quot;,&quot;open benchmark datasets&quot;]},&quot;publishedAt&quot;:&quot;2024-07-10T13:40:30.000Z&quot;,&quot;title&quot;:&quot;Toto: Time Series Optimized Transformer for Observability&quot;,&quot;summary&quot;:&quot;This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/645e9d6d9c8e15af60a7d44f/7XPYOicouddIn7uG2CoBR.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07874.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;645e9d6d9c8e15af60a7d44f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645e9d6d9c8e15af60a7d44f/uCuZRH2YcYktidW-Re9Xp.png&quot;,&quot;fullname&quot;:&quot;Emaad Khwaja&quot;,&quot;name&quot;:&quot;Emaad&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09435&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;66948ba1df5d51613344d461&quot;,&quot;name&quot;:&quot;Jessica Echterhoff&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d462&quot;,&quot;name&quot;:&quot;Fartash Faghri&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d463&quot;,&quot;name&quot;:&quot;Raviteja Vemulapalli&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d464&quot;,&quot;name&quot;:&quot;Ting-Yao Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d465&quot;,&quot;name&quot;:&quot;Chun-Liang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d466&quot;,&quot;name&quot;:&quot;Oncel Tuzel&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ba1df5d51613344d467&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;653a8e65c75dc136bfb5b0f8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ef252794236ce0fe2debf12773c95bb2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hadi Pouransari&quot;,&quot;user&quot;:&quot;hpouransari&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Hadi Pouransari&quot;,&quot;status&quot;:&quot;extracted_pending&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T02:38:26.354Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T17:12:48.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T01:08:51.476Z&quot;,&quot;title&quot;:&quot;MUSCLE: A Model Update Strategy for Compatible LLM Evolution&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Models (LLMs) are frequently updated due to data or\narchitecture changes to improve their performance. When updating models,\ndevelopers often focus on increasing overall performance metrics with less\nemphasis on being compatible with previous model versions. However, users often\nbuild a mental model of the functionality and capabilities of a particular\nmachine learning model they are interacting with. They have to adapt their\nmental model with every update -- a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned downstream task adapters rely on\npretrained LLM base models. When these base models are updated, these\nuser-facing downstream task models experience instance regression or negative\nflips -- previously correct instances are now predicted incorrectly. This\nhappens even when the downstream task training procedures remain identical. Our\nwork aims to provide seamless model updates to a user in two ways. First, we\nprovide evaluation metrics for a notion of compatibility to prior model\nversions, specifically for generative tasks but also applicable for\ndiscriminative tasks. We observe regression and inconsistencies between\ndifferent model versions on a diverse set of tasks and model updates. Second,\nwe propose a training strategy to minimize the number of inconsistencies in\nmodel updates, involving training of a compatibility model that can enhance\ntask fine-tuned language models. We reduce negative flips -- instances where a\nprior model version was correct, but a new model incorrect -- by up to 40% from\nLlama 1 to Llama 2.&quot;,&quot;upvotes&quot;:23,&quot;discussionId&quot;:&quot;66948ba2df5d51613344d4c2&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models (LLMs)&quot;,&quot;fine-tuned downstream task adapters&quot;,&quot;pretrained LLM base models&quot;,&quot;instance regression&quot;,&quot;negative flips&quot;,&quot;compatibility metrics&quot;,&quot;generative tasks&quot;,&quot;discriminative tasks&quot;,&quot;compatibility model&quot;,&quot;negative flips&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T13:12:48.000Z&quot;,&quot;title&quot;:&quot;MUSCLE: A Model Update Strategy for Compatible LLM Evolution&quot;,&quot;summary&quot;:&quot;Large Language Models (LLMs) are frequently updated due to data or\narchitecture changes to improve their performance. When updating models,\ndevelopers often focus on increasing overall performance metrics with less\nemphasis on being compatible with previous model versions. However, users often\nbuild a mental model of the functionality and capabilities of a particular\nmachine learning model they are interacting with. They have to adapt their\nmental model with every update -- a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned downstream task adapters rely on\npretrained LLM base models. When these base models are updated, these\nuser-facing downstream task models experience instance regression or negative\nflips -- previously correct instances are now predicted incorrectly. This\nhappens even when the downstream task training procedures remain identical. Our\nwork aims to provide seamless model updates to a user in two ways. First, we\nprovide evaluation metrics for a notion of compatibility to prior model\nversions, specifically for generative tasks but also applicable for\ndiscriminative tasks. We observe regression and inconsistencies between\ndifferent model versions on a diverse set of tasks and model updates. Second,\nwe propose a training strategy to minimize the number of inconsistencies in\nmodel updates, involving training of a compatibility model that can enhance\ntask fine-tuned language models. We reduce negative flips -- instances where a\nprior model version was correct, but a new model incorrect -- by up to 40% from\nLlama 1 to Llama 2.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09435.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.08770&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64ec52887e69909b1ad1ff0b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/30bb6fda51093fc71e80d0313c13d15e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lucy Wang&quot;,&quot;user&quot;:&quot;Lucywang720&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Huanqian Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T08:58:14.171Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;649d475111592b1a765ac1a3&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yang Yue&quot;,&quot;user&quot;:&quot;Yang130&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yang Yue&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-11-05T07:59:01.337Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5c&quot;,&quot;name&quot;:&quot;Rui Lu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5d&quot;,&quot;name&quot;:&quot;Jingxin Shi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;630482fbce6b12280b18971d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b07f31fd970d736bdf574d56da7a5634.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andrew Zhao&quot;,&quot;user&quot;:&quot;andrewzh&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Andrew Zhao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-16T20:16:22.404Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb5f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6486dde1f74857df3f1a5828&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shenzhi Wang&quot;,&quot;user&quot;:&quot;shenzhi-wang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shenzhi Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-18T09:08:06.670Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb60&quot;,&quot;name&quot;:&quot;Shiji Song&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694bd9b64dc88cf5e83fb61&quot;,&quot;name&quot;:&quot;Gao Huang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-11T17:52:03.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T07:37:13.056Z&quot;,&quot;title&quot;:&quot;Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64ec52887e69909b1ad1ff0b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/30bb6fda51093fc71e80d0313c13d15e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lucy Wang&quot;,&quot;user&quot;:&quot;Lucywang720&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current methods for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computation cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking. Specifically, for a\nbehavior that we aim to avoid, we employ a linear classifier, which we term the\nbehavior probe, to classify binary behavior labels within the hidden state\nspace of the LLM. Using this probe, we introduce an algorithm to identify a\ncritical subset of LLM parameters that significantly influence this targeted\nbehavior. Then we directly edit these selected parameters by shifting them\ntowards the behavior probe. Such a direct parameter editing method necessitates\nonly inference-level computational resources. Experiments demonstrate that in\nthe representative detoxification task, our approach achieves reductions of up\nto 90.0\\% in toxicity on the RealToxicityPrompts dataset and 49.2\\% on ToxiGen,\nwhile maintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics. Our code is available at\nhttps://github.com/lucywang720/model-surgery.&quot;,&quot;upvotes&quot;:21,&quot;discussionId&quot;:&quot;6694bd9c64dc88cf5e83fbc6&quot;,&quot;ai_keywords&quot;:[&quot;Supervised Fine-Tuning (SFT)&quot;,&quot;Reinforcement Learning from Human Feedback (RLHF)&quot;,&quot;hidden state space&quot;,&quot;behavior probe&quot;,&quot;direct parameter editing&quot;,&quot;RealToxicityPrompts&quot;,&quot;ToxiGen&quot;]},&quot;publishedAt&quot;:&quot;2024-07-11T13:52:03.000Z&quot;,&quot;title&quot;:&quot;Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing&quot;,&quot;summary&quot;:&quot;Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current methods for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computation cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking. Specifically, for a\nbehavior that we aim to avoid, we employ a linear classifier, which we term the\nbehavior probe, to classify binary behavior labels within the hidden state\nspace of the LLM. Using this probe, we introduce an algorithm to identify a\ncritical subset of LLM parameters that significantly influence this targeted\nbehavior. Then we directly edit these selected parameters by shifting them\ntowards the behavior probe. Such a direct parameter editing method necessitates\nonly inference-level computational resources. Experiments demonstrate that in\nthe representative detoxification task, our approach achieves reductions of up\nto 90.0\\% in toxicity on the RealToxicityPrompts dataset and 49.2\\% on ToxiGen,\nwhile maintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics. Our code is available at\nhttps://github.com/lucywang720/model-surgery.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08770.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64ec52887e69909b1ad1ff0b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/30bb6fda51093fc71e80d0313c13d15e.svg&quot;,&quot;fullname&quot;:&quot;Lucy Wang&quot;,&quot;name&quot;:&quot;Lucywang720&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09276&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;669488b7ce544964b27167a2&quot;,&quot;name&quot;:&quot;Pascal Pfeiffer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488b7ce544964b27167a3&quot;,&quot;name&quot;:&quot;Philipp Singer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488b7ce544964b27167a4&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;643f884da649e1a499446c11&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/643f884da649e1a499446c11/4lmRszXeD1S62sfwUHg2D.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yauhen Babakhin&quot;,&quot;user&quot;:&quot;ybabakhin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yauhen Babakhin&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-02-13T20:36:31.298Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488b7ce544964b27167a5&quot;,&quot;name&quot;:&quot;Gabor Fodor&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488b7ce544964b27167a6&quot;,&quot;name&quot;:&quot;Nischay Dhankhar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669488b7ce544964b27167a7&quot;,&quot;name&quot;:&quot;Sri Satish Ambati&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T14:09:40.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T00:56:18.217Z&quot;,&quot;title&quot;:&quot;H2O-Danube3 Technical Report&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;669488b8ce544964b27167d3&quot;,&quot;ai_keywords&quot;:[&quot;language models&quot;,&quot;H2O-Danube3-4B&quot;,&quot;H2O-Danube3-500M&quot;,&quot;Web data&quot;,&quot;pre-trained&quot;,&quot;supervised tuning&quot;,&quot;chat version&quot;,&quot;semantic scene synthesis&quot;,&quot;local inference&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T10:09:40.000Z&quot;,&quot;title&quot;:&quot;H2O-Danube3 Technical Report&quot;,&quot;summary&quot;:&quot;We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09276.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09388&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437bdd&quot;,&quot;name&quot;:&quot;Graham Todd&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437bde&quot;,&quot;name&quot;:&quot;Alexander Padula&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437bdf&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;65b1a3b6b389ca2de1296658&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a78a9da6f94fc5683aa3184cd5975253.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Matthew Stephenson&quot;,&quot;user&quot;:&quot;step0330&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Matthew Stephenson&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-08-02T07:35:05.622Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437be0&quot;,&quot;name&quot;:&quot;Éric Piette&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437be1&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62cb2f860d72a9876be10bd5&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1657483110003-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dennis Soemers&quot;,&quot;user&quot;:&quot;DennisSoemers&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Dennis J. N. J. Soemers&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-16T20:16:27.311Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948a7b3774cc5e5c437be2&quot;,&quot;name&quot;:&quot;Julian Togelius&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T16:08:44.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T01:04:03.646Z&quot;,&quot;title&quot;:&quot;GAVEL: Generating Games Via Evolution and Language Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.&quot;,&quot;upvotes&quot;:17,&quot;discussionId&quot;:&quot;66948a7c3774cc5e5c437c37&quot;,&quot;ai_keywords&quot;:[&quot;Ludii game description language&quot;,&quot;large language models&quot;,&quot;evolutionary computation&quot;,&quot;game-rule&quot;,&quot;domain-specific heuristics&quot;,&quot;game mechanics&quot;,&quot;encoder styles&quot;,&quot;modes of play&quot;,&quot;potential rules space&quot;,&quot;Ludii portal&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T12:08:44.000Z&quot;,&quot;title&quot;:&quot;GAVEL: Generating Games Via Evolution and Language Models&quot;,&quot;summary&quot;:&quot;Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09388.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09298&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;669489afcab4aab691e57e1a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;644b983f0fbe4830f192c4f5&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/644b983f0fbe4830f192c4f5/4eX4OS0gnXGqLu3eaMml_.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qi Sun&quot;,&quot;user&quot;:&quot;lfsm&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Qi Sun&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-08-05T09:02:32.335Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669489afcab4aab691e57e1b&quot;,&quot;name&quot;:&quot;Marc Pickett&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669489afcab4aab691e57e1c&quot;,&quot;name&quot;:&quot;Aakash Kumar Nain&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669489afcab4aab691e57e1d&quot;,&quot;name&quot;:&quot;Llion Jones&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T14:31:05.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T01:00:23.298Z&quot;,&quot;title&quot;:&quot;Transformer Layers as Painters&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.&quot;,&quot;upvotes&quot;:15,&quot;discussionId&quot;:&quot;669489b0cab4aab691e57e71&quot;,&quot;ai_keywords&quot;:[&quot;transformers&quot;,&quot;pretrained transformers&quot;,&quot;frozen models&quot;,&quot;lower layers&quot;,&quot;final layers&quot;,&quot;middle layers&quot;,&quot;latency&quot;,&quot;skipping layers&quot;,&quot;parallelization&quot;,&quot;robustness&quot;,&quot;accuracy&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T10:31:05.000Z&quot;,&quot;title&quot;:&quot;Transformer Layers as Painters&quot;,&quot;summary&quot;:&quot;Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09298.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09473&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6694866b8cffcbe1be0eb717&quot;,&quot;name&quot;:&quot;Sahil Jain&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694866b8cffcbe1be0eb718&quot;,&quot;name&quot;:&quot;Avik Kuthiala&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694866b8cffcbe1be0eb719&quot;,&quot;name&quot;:&quot;Prabhdeep Singh Sethi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694866b8cffcbe1be0eb71a&quot;,&quot;name&quot;:&quot;Prakanshul Saxena&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T17:55:08.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T00:46:24.612Z&quot;,&quot;title&quot;:&quot;StyleSplat: 3D Object Style Transfer with Gaussian Splatting&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.&quot;,&quot;upvotes&quot;:12,&quot;discussionId&quot;:&quot;669486718cffcbe1be0eb98e&quot;,&quot;ai_keywords&quot;:[&quot;radiance fields&quot;,&quot;3D Gaussians&quot;,&quot;StyleSplat&quot;,&quot;photorealistic representation&quot;,&quot;3D Gaussian splatting&quot;,&quot;nearest-neighbor feature matching loss&quot;,&quot;spherical harmonic coefficients&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T13:55:08.000Z&quot;,&quot;title&quot;:&quot;StyleSplat: 3D Object Style Transfer with Gaussian Splatting&quot;,&quot;summary&quot;:&quot;Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09473.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09413&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6694e325c5f4e75ee510029d&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64aec10a9135f79b09117476&quot;,&quot;avatarUrl&quot;:&quot;/avatars/88a6bcfeee99c12496940575a2fc65e9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shraman Pramanick&quot;,&quot;user&quot;:&quot;Shraman&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shraman Pramanick&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-22T07:10:54.292Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694e325c5f4e75ee510029e&quot;,&quot;name&quot;:&quot;Rama Chellappa&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694e325c5f4e75ee510029f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;646125a5933afb0106a9dabb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ccef359e4442f560a87fd66228e54c8b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Subhashini&quot;,&quot;user&quot;:&quot;vsubhashini&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Subhashini Venugopalan&quot;,&quot;status&quot;:&quot;extracted_pending&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T08:51:53.389Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T16:37:59.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T07:25:21.663Z&quot;,&quot;title&quot;:&quot;SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60107b385ac3e86b3ea4fc34&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Daniel van Strien&quot;,&quot;user&quot;:&quot;davanstrien&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.&quot;,&quot;upvotes&quot;:11,&quot;discussionId&quot;:&quot;6694e329c5f4e75ee5100430&quot;,&quot;ai_keywords&quot;:[&quot;multimodal large language models (MLLMs)&quot;,&quot;SPIQA (Scientific Paper Image Question Answering)&quot;,&quot;Chain-of-Thought (CoT) evaluation strategy&quot;,&quot;in-context retrieval&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T12:37:59.000Z&quot;,&quot;title&quot;:&quot;SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers&quot;,&quot;summary&quot;:&quot;Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09413.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60107b385ac3e86b3ea4fc34&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg&quot;,&quot;fullname&quot;:&quot;Daniel van Strien&quot;,&quot;name&quot;:&quot;davanstrien&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:573},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09072&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6694d14ccab4aab691fa1c71&quot;,&quot;name&quot;:&quot;Xiangkun Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694d14ccab4aab691fa1c72&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63184606ab1bf092d6f059be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b49d125d112a01106b48a2fcda481c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tong He&quot;,&quot;user&quot;:&quot;hetong007&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tong He&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T08:58:03.349Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694d14ccab4aab691fa1c73&quot;,&quot;name&quot;:&quot;David Wipf&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T07:52:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T06:10:22.376Z&quot;,&quot;title&quot;:&quot;New Desiderata for Direct Preference Optimization&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;63184606ab1bf092d6f059be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b49d125d112a01106b48a2fcda481c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tong He&quot;,&quot;user&quot;:&quot;hetong007&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.&quot;,&quot;upvotes&quot;:11,&quot;discussionId&quot;:&quot;6694d14dcab4aab691fa1cc2&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning with human feedback (RLHF)&quot;,&quot;direct preference optimization (DPO)&quot;,&quot;live video processing&quot;,&quot;reparameterization&quot;,&quot;pre-trained model&quot;,&quot;human preferences&quot;,&quot;training objective&quot;,&quot;regularization&quot;,&quot;constraints&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T03:52:32.000Z&quot;,&quot;title&quot;:&quot;New Desiderata for Direct Preference Optimization&quot;,&quot;summary&quot;:&quot;Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09072.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;63184606ab1bf092d6f059be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b49d125d112a01106b48a2fcda481c3e.svg&quot;,&quot;fullname&quot;:&quot;Tong He&quot;,&quot;name&quot;:&quot;hetong007&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.08892&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;66948ea3eb7d71adc7547180&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;646fc09c850a938d6c54c2eb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f4793a7367d372def34014551db48be2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sid Jha&quot;,&quot;user&quot;:&quot;sidjha1&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Siddharth Jha&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T08:58:24.805Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ea3eb7d71adc7547181&quot;,&quot;name&quot;:&quot;Lutfi Eren Erdogan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ea3eb7d71adc7547182&quot;,&quot;name&quot;:&quot;Sehoon Kim&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ea3eb7d71adc7547183&quot;,&quot;name&quot;:&quot;Kurt Keutzer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66948ea3eb7d71adc7547184&quot;,&quot;name&quot;:&quot;Amir Gholami&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/646fc09c850a938d6c54c2eb/L9MGeiHWGSN1nKBVZ7Qm9.png&quot;],&quot;publishedAt&quot;:&quot;2024-07-11T23:34:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T05:04:38.218Z&quot;,&quot;title&quot;:&quot;Characterizing Prompt Compression Methods for Long Context Inference&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;646fc09c850a938d6c54c2eb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f4793a7367d372def34014551db48be2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sid Jha&quot;,&quot;user&quot;:&quot;sidjha1&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.&quot;,&quot;upvotes&quot;:11,&quot;discussionId&quot;:&quot;66948ea4eb7d71adc75471d5&quot;,&quot;ai_keywords&quot;:[&quot;prompt compression&quot;,&quot;extractive compression&quot;,&quot;summarization-based abstractive compression&quot;,&quot;token pruning&quot;]},&quot;publishedAt&quot;:&quot;2024-07-11T19:34:32.000Z&quot;,&quot;title&quot;:&quot;Characterizing Prompt Compression Methods for Long Context Inference&quot;,&quot;summary&quot;:&quot;Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/646fc09c850a938d6c54c2eb/L9MGeiHWGSN1nKBVZ7Qm9.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08892.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;646fc09c850a938d6c54c2eb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f4793a7367d372def34014551db48be2.svg&quot;,&quot;fullname&quot;:&quot;Sid Jha&quot;,&quot;name&quot;:&quot;sidjha1&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09012&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;669485a33ceee2f1203f6bef&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6100cc3189e20ab255c4a243&quot;,&quot;avatarUrl&quot;:&quot;/avatars/36a30c0f9e03488b24313be4c1a6ed59.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jeongho Kim&quot;,&quot;user&quot;:&quot;rlawjdghek&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jeongho Kim&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-18T09:08:08.802Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669485a33ceee2f1203f6bf0&quot;,&quot;name&quot;:&quot;Min-Jung Kim&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669485a33ceee2f1203f6bf1&quot;,&quot;name&quot;:&quot;Junsoo Lee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;669485a33ceee2f1203f6bf2&quot;,&quot;name&quot;:&quot;Jaegul Choo&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T06:02:13.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T00:43:02.039Z&quot;,&quot;title&quot;:&quot;TCAN: Animating Human Images with Temporally Consistent Pose Guidance\n  using Diffusion Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/&quot;,&quot;upvotes&quot;:10,&quot;discussionId&quot;:&quot;669485a53ceee2f1203f6cc1&quot;,&quot;ai_keywords&quot;:[&quot;Pose-driven human-image animation diffusion models&quot;,&quot;Realistic human video synthesis&quot;,&quot;Temporally consistent animation&quot;,&quot;Off-the-shelf pose detectors&quot;,&quot;TCAN&quot;,&quot;ControlNet&quot;,&quot;LoRA&quot;,&quot;UNet layers&quot;,&quot;Latent space&quot;,&quot;Appearance features&quot;,&quot;Temporal layer&quot;,&quot;Outliers of the pose detector&quot;,&quot;Attention maps&quot;,&quot;Temperature map&quot;,&quot;Chibi&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T02:02:13.000Z&quot;,&quot;title&quot;:&quot;TCAN: Animating Human Images with Temporally Consistent Pose Guidance\n  using Diffusion Models&quot;,&quot;summary&quot;:&quot;Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09012.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09732&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6695c32430bd2a19adece3f5&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6531a65daed617662c7f1007&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xilin Jiang&quot;,&quot;user&quot;:&quot;xi-j&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xilin Jiang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-02-26T08:43:23.511Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6695c32430bd2a19adece3f6&quot;,&quot;name&quot;:&quot;Yinghao Aaron Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6695c32430bd2a19adece3f7&quot;,&quot;name&quot;:&quot;Adrian Nicolas Florea&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6695c32430bd2a19adece3f8&quot;,&quot;name&quot;:&quot;Cong Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6695c32430bd2a19adece3f9&quot;,&quot;name&quot;:&quot;Nima Mesgarani&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-13T00:35:21.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T23:18:08.295Z&quot;,&quot;title&quot;:&quot;Speech Slytherin: Examining the Performance and Efficiency of Mamba for\n  Speech Separation, Recognition, and Synthesis&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6531a65daed617662c7f1007&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xilin Jiang&quot;,&quot;user&quot;:&quot;xi-j&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;It is too early to conclude that Mamba is a better alternative to\ntransformers for speech before comparing Mamba with transformers in terms of\nboth performance and efficiency in multiple speech-related tasks. To reach this\nconclusion, we propose and evaluate three models for three tasks: Mamba-TasNet\nfor speech separation, ConMamba for speech recognition, and VALL-M for speech\nsynthesis. We compare them with transformers of similar sizes in performance,\nmemory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable\nor higher performance than their transformer counterparts: Sepformer,\nConformer, and VALL-E. They are more efficient than transformers in memory and\nspeed for speech longer than a threshold duration, inversely related to the\nresolution of a speech token. Mamba for separation is the most efficient, and\nMamba for recognition is the least. Further, we show that Mamba is not more\nefficient than transformer for speech shorter than the threshold duration and\nperforms worse in models that require joint modeling of text and speech, such\nas cross or masked attention of two inputs. Therefore, we argue that the\nsuperiority of Mamba or transformer depends on particular problems and models.\nCode available at https://github.com/xi-j/Mamba-TasNet and\nhttps://github.com/xi-j/Mamba-ASR.&quot;,&quot;upvotes&quot;:9,&quot;discussionId&quot;:&quot;6695c32530bd2a19adece46c&quot;,&quot;ai_keywords&quot;:[&quot;Mamba-TasNet&quot;,&quot;ConMamba&quot;,&quot;VALL-M&quot;,&quot;Sepformer&quot;,&quot;Conformer&quot;,&quot;VALL-E&quot;,&quot;speech separation&quot;,&quot;speech recognition&quot;,&quot;speech synthesis&quot;,&quot;speech token&quot;,&quot;cross attention&quot;,&quot;masked attention&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T20:35:21.000Z&quot;,&quot;title&quot;:&quot;Speech Slytherin: Examining the Performance and Efficiency of Mamba for\n  Speech Separation, Recognition, and Synthesis&quot;,&quot;summary&quot;:&quot;It is too early to conclude that Mamba is a better alternative to\ntransformers for speech before comparing Mamba with transformers in terms of\nboth performance and efficiency in multiple speech-related tasks. To reach this\nconclusion, we propose and evaluate three models for three tasks: Mamba-TasNet\nfor speech separation, ConMamba for speech recognition, and VALL-M for speech\nsynthesis. We compare them with transformers of similar sizes in performance,\nmemory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable\nor higher performance than their transformer counterparts: Sepformer,\nConformer, and VALL-E. They are more efficient than transformers in memory and\nspeed for speech longer than a threshold duration, inversely related to the\nresolution of a speech token. Mamba for separation is the most efficient, and\nMamba for recognition is the least. Further, we show that Mamba is not more\nefficient than transformer for speech shorter than the threshold duration and\nperforms worse in models that require joint modeling of text and speech, such\nas cross or masked attention of two inputs. Therefore, we argue that the\nsuperiority of Mamba or transformer depends on particular problems and models.\nCode available at https://github.com/xi-j/Mamba-TasNet and\nhttps://github.com/xi-j/Mamba-ASR.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09732.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6531a65daed617662c7f1007&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg&quot;,&quot;fullname&quot;:&quot;Xilin Jiang&quot;,&quot;name&quot;:&quot;xi-j&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2406.02265&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;668f02f92eb49d45ac6a4f2a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;619b506f70d03780cbec5806&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;wenyan li&quot;,&quot;user&quot;:&quot;lyan62&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wenyan Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T09:01:01.526Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;668f02f92eb49d45ac6a4f2b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6686658221f096c7fb4e0b24&quot;,&quot;avatarUrl&quot;:&quot;/avatars/581f5d56b393b95c74f38a06f6bb4f69.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiaang Li&quot;,&quot;user&quot;:&quot;jaagli&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jiaang Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-11T08:26:59.087Z&quot;,&quot;hidden&quot;:true},{&quot;_id&quot;:&quot;668f02f92eb49d45ac6a4f2c&quot;,&quot;name&quot;:&quot;Rita Ramos&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;668f02f92eb49d45ac6a4f2d&quot;,&quot;name&quot;:&quot;Raphael Tang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;668f02f92eb49d45ac6a4f2e&quot;,&quot;name&quot;:&quot;Desmond Elliott&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-06-04T12:41:54.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T00:49:17.917Z&quot;,&quot;title&quot;:&quot;Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.&quot;,&quot;upvotes&quot;:7,&quot;discussionId&quot;:&quot;668f02fa2eb49d45ac6a4f89&quot;,&quot;ai_keywords&quot;:[&quot;retrieval-augmented models&quot;,&quot;image captioning&quot;,&quot;retrieval models&quot;,&quot;domain-transfer capabilities&quot;,&quot;token attribution&quot;,&quot;caption generation&quot;,&quot;model sensitivity&quot;,&quot;majority tokens&quot;,&quot;diverse sets&quot;]},&quot;publishedAt&quot;:&quot;2024-06-04T08:41:54.000Z&quot;,&quot;title&quot;:&quot;Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning&quot;,&quot;summary&quot;:&quot;Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02265.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:true,&quot;isMod&quot;:false,&quot;followerCount&quot;:6373},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.09121&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;66949c294f892948c3a09b23&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6447843530fa4ecb85ddc889&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a97c4970a1a179ee8a2e2e6ab8f995f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Youliang Yuan&quot;,&quot;user&quot;:&quot;Youliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Youliang Yuan&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-07-15T08:58:19.646Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b24&quot;,&quot;name&quot;:&quot;Wenxiang Jiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b25&quot;,&quot;name&quot;:&quot;Wenxuan Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b26&quot;,&quot;name&quot;:&quot;Jen-tse Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b27&quot;,&quot;name&quot;:&quot;Jiahao Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b28&quot;,&quot;name&quot;:&quot;Tian Liang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b29&quot;,&quot;name&quot;:&quot;Pinjia He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;66949c294f892948c3a09b2a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;67485743561b1e6f9579389f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8a4cc63bd7be388010bc329bb74582a1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaopeng Tu&quot;,&quot;user&quot;:&quot;zptu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhaopeng Tu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-11-28T11:44:10.045Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-12T09:36:33.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T07:40:38.073Z&quot;,&quot;title&quot;:&quot;Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n  Refusal Training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6447843530fa4ecb85ddc889&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a97c4970a1a179ee8a2e2e6ab8f995f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Youliang Yuan&quot;,&quot;user&quot;:&quot;Youliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.&quot;,&quot;upvotes&quot;:6,&quot;discussionId&quot;:&quot;66949c2a4f892948c3a09b80&quot;,&quot;ai_keywords&quot;:[&quot;Decoupled Refusal Training (DeRTa)&quot;,&quot;Maximum Likelihood Estimation (MLE)&quot;,&quot;Harmful Response Prefix&quot;,&quot;Reinforced Transition Optimization (RTO)&quot;,&quot;LLaMA3&quot;,&quot;Mistral&quot;,&quot;CodeAttack&quot;,&quot;jailbroken GPT-4&quot;,&quot;LLaMA3-70B-Instruct&quot;]},&quot;publishedAt&quot;:&quot;2024-07-12T05:36:33.000Z&quot;,&quot;title&quot;:&quot;Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n  Refusal Training&quot;,&quot;summary&quot;:&quot;This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09121.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6447843530fa4ecb85ddc889&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a97c4970a1a179ee8a2e2e6ab8f995f6.svg&quot;,&quot;fullname&quot;:&quot;Youliang Yuan&quot;,&quot;name&quot;:&quot;Youliang&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2407.06397&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6694f7600e08a6505b344cf5&quot;,&quot;name&quot;:&quot;Diego Gomez&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694f7600e08a6505b344cf6&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63e0dbb35ba41def8794dab8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38a28103b15d93c7d9243bc5418e623e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Julien Philip&quot;,&quot;user&quot;:&quot;Jovphi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Julien Philip&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2024-09-17T15:08:21.742Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694f7600e08a6505b344cf7&quot;,&quot;name&quot;:&quot;Adrien Kaiser&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6694f7600e08a6505b344cf8&quot;,&quot;name&quot;:&quot;Élie Michel&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2024-07-08T21:10:31.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2024-07-15T08:56:27.731Z&quot;,&quot;title&quot;:&quot;RRM: Relightable assets using Radiance guided Material extraction&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;63e0dbb35ba41def8794dab8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38a28103b15d93c7d9243bc5418e623e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Julien Philip&quot;,&quot;user&quot;:&quot;Jovphi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Synthesizing NeRFs under arbitrary lighting has become a seminal problem in\nthe last few years. Recent efforts tackle the problem via the extraction of\nphysically-based parameters that can then be rendered under arbitrary lighting,\nbut they are limited in the range of scenes they can handle, usually\nmishandling glossy scenes. We propose RRM, a method that can extract the\nmaterials, geometry, and environment lighting of a scene even in the presence\nof highly reflective objects. Our method consists of a physically-aware\nradiance field representation that informs physically-based parameters, and an\nexpressive environment light structure based on a Laplacian Pyramid. We\ndemonstrate that our contributions outperform the state-of-the-art on parameter\nretrieval tasks, leading to high-fidelity relighting and novel view synthesis\non surfacic scenes.&quot;,&quot;upvotes&quot;:5,&quot;discussionId&quot;:&quot;6694f7660e08a6505b344e5d&quot;,&quot;ai_keywords&quot;:[&quot;radiance field representation&quot;,&quot;physically-based parameters&quot;,&quot;environment light structure&quot;,&quot;Laplacian Pyramid&quot;,&quot;parameter retrieval tasks&quot;,&quot;high-fidelity relighting&quot;,&quot;novel view synthesis&quot;]},&quot;publishedAt&quot;:&quot;2024-07-08T17:10:31.000Z&quot;,&quot;title&quot;:&quot;RRM: Relightable assets using Radiance guided Material extraction&quot;,&quot;summary&quot;:&quot;Synthesizing NeRFs under arbitrary lighting has become a seminal problem in\nthe last few years. Recent efforts tackle the problem via the extraction of\nphysically-based parameters that can then be rendered under arbitrary lighting,\nbut they are limited in the range of scenes they can handle, usually\nmishandling glossy scenes. We propose RRM, a method that can extract the\nmaterials, geometry, and environment lighting of a scene even in the presence\nof highly reflective objects. Our method consists of a physically-aware\nradiance field representation that informs physically-based parameters, and an\nexpressive environment light structure based on a Laplacian Pyramid. We\ndemonstrate that our contributions outperform the state-of-the-art on parameter\nretrieval tasks, leading to high-fidelity relighting and novel view synthesis\non surfacic scenes.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06397.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;63e0dbb35ba41def8794dab8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38a28103b15d93c7d9243bc5418e623e.svg&quot;,&quot;fullname&quot;:&quot;Julien Philip&quot;,&quot;name&quot;:&quot;Jovphi&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true}],&quot;prevDate&quot;:&quot;2024-07-12&quot;,&quot;publisher&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;periodType&quot;:&quot;day&quot;,&quot;nextDate&quot;:&quot;2024-07-16&quot;,&quot;query&quot;:{}}"><section class="container relative mb-20 mt-8 md:mt-14"><div class="mb-8 grid grid-cols-6 items-start md:mb-12 md:grid-cols-12 md:gap-x-4"><div class="order-1 col-span-5 md:order-none md:col-span-10 lg:col-span-8 xl:col-span-4 xl:pl-0"><div class="flex items-center gap-3"><h1 class="text-2xl font-bold md:text-3xl"><a href="/papers" class="hover:text-gray-600 dark:hover:text-gray-300">Daily Papers</a></h1>
				</div>
			<h2 class="whitespace-nowrap text-gray-500 xl:text-lg">by<a href="/akhaliq"><img alt="" class="mx-1.5 inline h-4" src="/front/assets/papers-by.png"><span class="underline">AK</span></a> and the research community
			</h2></div>

		<div class="order-1 col-span-1 flex justify-end pt-1 md:order-none md:col-span-2 md:mt-0 lg:col-span-4 xl:hidden"></div>

		<div class="order-3 col-span-6 mt-3 flex items-center self-start md:col-span-12 md:mt-4 lg:col-span-12 xl:order-none xl:col-span-4 xl:mt-0"><div class="relative z-1 flex w-full items-center"><svg class="absolute left-[1.1rem] select-none text-gray-700 dark:text-gray-300" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 11 11"><path fill="currentColor" d="M4.881 4.182c0 .101-.031.2-.087.283a.5.5 0 0 1-.242.18l-.65.217a1.3 1.3 0 0 0-.484.299 1.3 1.3 0 0 0-.298.484l-.222.639a.46.46 0 0 1-.18.242.5.5 0 0 1-.288.092.5.5 0 0 1-.294-.097.5.5 0 0 1-.175-.242l-.211-.644a1.26 1.26 0 0 0-.299-.48 1.14 1.14 0 0 0-.479-.298L.328 4.64a.48.48 0 0 1-.247-.18.515.515 0 0 1 .247-.758l.644-.21a1.28 1.28 0 0 0 .788-.789l.211-.634a.5.5 0 0 1 .165-.242.5.5 0 0 1 .283-.103.5.5 0 0 1 .294.083c.086.058.152.14.19.237l.217.659a1.28 1.28 0 0 0 .788.788l.644.222a.476.476 0 0 1 .237.18.5.5 0 0 1 .092.288"></path><path fill="currentColor" d="M10.031 7.458a.5.5 0 0 1-.098.314.5.5 0 0 1-.267.196l-.881.293c-.272.09-.519.242-.721.443a1.8 1.8 0 0 0-.443.721l-.31.876a.5.5 0 0 1-.185.263.56.56 0 0 1-.319.098.515.515 0 0 1-.515-.366l-.294-.88a1.8 1.8 0 0 0-.443-.722c-.204-.2-.45-.353-.72-.448l-.881-.288a.57.57 0 0 1-.263-.191.56.56 0 0 1-.014-.64.5.5 0 0 1 .271-.194l.886-.294A1.82 1.82 0 0 0 6.01 5.465l.293-.87a.515.515 0 0 1 .49-.377c.11 0 .219.03.314.088a.56.56 0 0 1 .206.263l.298.896a1.82 1.82 0 0 0 1.175 1.174l.875.31a.5.5 0 0 1 .263.195c.07.09.108.2.108.314"></path><path fill="currentColor" d="M7.775 1.684a.5.5 0 0 0 .088-.262.45.45 0 0 0-.088-.263.5.5 0 0 0-.21-.155L7.24.896a.5.5 0 0 1-.165-.103.5.5 0 0 1-.103-.17l-.108-.33a.5.5 0 0 0-.165-.21A.5.5 0 0 0 6.426 0a.5.5 0 0 0-.252.098.5.5 0 0 0-.145.206l-.108.32a.5.5 0 0 1-.103.17.5.5 0 0 1-.17.102L5.334 1a.45.45 0 0 0-.216.155.5.5 0 0 0-.088.262c0 .094.029.186.083.263a.5.5 0 0 0 .216.16l.32.103q.095.03.164.103a.37.37 0 0 1 .103.165l.108.319c.031.09.088.17.165.227a.56.56 0 0 0 .252.077.42.42 0 0 0 .268-.093.5.5 0 0 0 .15-.2l.113-.325a.43.43 0 0 1 .268-.268l.32-.108a.42.42 0 0 0 .215-.155"></path></svg>
				<input type="text" class="hidden" name="daily-papers-search-fake">
				
				<input type="search" name="daily-papers-search" autocomplete="off" class="shadow-alternate-sm max-w-full flex-1 rounded-full border-gray-200 py-2 pl-10 pr-12 placeholder:text-gray-400 focus:ring-2 focus:ring-blue-500/30 dark:bg-gray-850 dark:focus:ring-blue-500 svelte-hhewj3" placeholder="Search any paper with AI" value="" maxlength="250">
				<button class="absolute grid size-7 place-items-center rounded-full border bg-gray-200 transition-all hover:brightness-95 dark:border-gray-700 dark:bg-gray-800 dark:hover:brightness-110 right-2"><svg class="text-sm text-gray-600" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1.1em" height="1.1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M24 9.4L22.6 8L16 14.6L9.4 8L8 9.4l6.6 6.6L8 22.6L9.4 24l6.6-6.6l6.6 6.6l1.4-1.4l-6.6-6.6L24 9.4z" fill="currentColor"></path></svg></button>
				<button class="dice-container absolute right-2 grid !size-7 place-items-center rounded-full border bg-gray-200 transition-all dark:border-gray-700 dark:bg-gray-800 svelte-1bquw"><div class="dice svelte-1bquw" style="transform: rotateX(-25deg)
                  rotateY(45deg)
                  rotateZ(0deg);"><div class="face front svelte-1bquw"></div>
		<div class="face up svelte-1bquw"></div>
		<div class="face left svelte-1bquw"></div>
		<div class="face right svelte-1bquw"></div>
		<div class="face bottom svelte-1bquw"></div>
		<div class="face back svelte-1bquw"></div></div>
</button></div></div>

		<div class="order-2 col-span-6 mt-3 flex w-full justify-center md:col-span-12 md:mt-4 lg:col-span-12 xl:order-none xl:col-span-4 xl:mt-0 xl:items-center xl:justify-end"><div class="flex w-full items-center justify-between w-full xl:w-auto"><ul class="flex gap-1 text-sm  "><li><button class="flex items-center whitespace-nowrap rounded-lg px-2  bg-black text-white dark:bg-gray-800">Daily
				</button>
		</li><li><button class="flex items-center whitespace-nowrap rounded-lg px-2  text-gray-500 hover:bg-gray-100 hover:text-gray-700 dark:hover:bg-gray-900 dark:hover:text-gray-300">Weekly
				</button>
		</li><li><button class="flex items-center whitespace-nowrap rounded-lg px-2  text-gray-500 hover:bg-gray-100 hover:text-gray-700 dark:hover:bg-gray-900 dark:hover:text-gray-300">Monthly
				</button>
		</li></ul>

	<div class="ml-6 flex items-center overflow-hidden rounded-lg bg-gray-100 dark:bg-gray-800"><a href="/papers/date/2024-07-12" class="flex size-8 items-center justify-center bg-gray-200 text-gray-800 hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-200 dark:hover:bg-gray-600 "><svg class="h-2" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a>
		<div class="w-24 whitespace-nowrap text-center text-sm font-semibold text-gray-900 dark:text-gray-100">Jul 15</div>
		<a href="/papers/date/2024-07-16" class="flex size-8 items-center justify-center bg-gray-200 text-gray-800 hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-200 dark:hover:bg-gray-600 "><svg class="h-2 rotate-180" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 10 10"><path d="M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z" fill="currentColor"></path></svg></a></div></div></div></div>

	<div class="relative grid grid-cols-1 gap-5 lg:grid-cols-2 xl:grid-cols-3"><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09025" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09025.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09025" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">135</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09025" class="line-clamp-3 cursor-pointer text-balance">SpreadsheetLLM: Encoding Spreadsheets for Large Language Models</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09025" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shiyu Xia" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Junyu Xiong" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Haoyu Dong" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jianbo Zhao" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yuzhang Tian" style="content-visibility:auto;">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								11 authors
							</div></li></ul></a>
					<a href="/papers/2407.09025#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							18</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09450" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09450.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09450" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">62</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09450" class="line-clamp-3 cursor-pointer text-balance">Human-like Episodic Memory for Infinite Context LLMs</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09450" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hba123" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="glampouras" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/95ec51e5408888d33b07696301ad34b6.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="fenchri" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/29aeb53c878c047a6c762d7449a50bf1.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="m84366023" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/85dcb534ff080ee1ca52d82eb41e1fc4.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="zfountas" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qOQW6D05xIM1HPc-S_adk.png">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								7 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.09450#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								6</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.07874" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07874.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/645e9d6d9c8e15af60a7d44f/uCuZRH2YcYktidW-Re9Xp.png" crossorigin="anonymous">
			Emaad</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.07874" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">32</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.07874" class="line-clamp-3 cursor-pointer text-balance">Toto: Time Series Optimized Transformer for Observability</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.07874" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Elise Ramé" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Charles Masson" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kan Wang" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ben Cohen" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Emaad" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/645e9d6d9c8e15af60a7d44f/uCuZRH2YcYktidW-Re9Xp.png">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								7 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.07874#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								3</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09435" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09435.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09435" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">23</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09435" class="line-clamp-3 cursor-pointer text-balance">MUSCLE: A Model Update Strategy for Compatible LLM Evolution</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09435" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Ting-Yao Hu" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Raviteja Vemulapalli" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Fartash Faghri" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jessica Echterhoff" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hpouransari" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ef252794236ce0fe2debf12773c95bb2.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								7 authors
							</div></li></ul></a>
					<a href="/papers/2407.09435#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.08770" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08770.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/30bb6fda51093fc71e80d0313c13d15e.svg" crossorigin="anonymous">
			Lucywang720</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.08770" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">21</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.08770" class="line-clamp-3 cursor-pointer text-balance">Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.08770" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Rui Lu" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="shenzhi-wang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="andrewzh" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b07f31fd970d736bdf574d56da7a5634.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yang130" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Lucywang720" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/30bb6fda51093fc71e80d0313c13d15e.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								8 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.08770#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								4</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09276" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09276.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09276" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">20</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09276" class="line-clamp-3 cursor-pointer text-balance">H2O-Danube3 Technical Report</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09276" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Nischay Dhankhar" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Gabor Fodor" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Philipp Singer" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Pascal Pfeiffer" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="ybabakhin" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/643f884da649e1a499446c11/4lmRszXeD1S62sfwUHg2D.jpeg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								6 authors
							</div></li></ul></a>
					<a href="/papers/2407.09276#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09388" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09388.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09388" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">17</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09388" class="line-clamp-3 cursor-pointer text-balance">GAVEL: Generating Games Via Evolution and Language Models</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09388" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Éric Piette" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Alexander Padula" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Graham Todd" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="DennisSoemers" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1657483110003-noauth.jpeg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="step0330" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/a78a9da6f94fc5683aa3184cd5975253.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								6 authors
							</div></li></ul></a>
					<a href="/papers/2407.09388#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09298" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09298.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09298" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">15</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09298" class="line-clamp-3 cursor-pointer text-balance">Transformer Layers as Painters</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09298" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Llion Jones" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Aakash Kumar Nain" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Marc Pickett" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lfsm" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/644b983f0fbe4830f192c4f5/4eX4OS0gnXGqLu3eaMml_.png">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								4 authors
							</div></li></ul></a>
					<a href="/papers/2407.09298#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09473" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09473.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09473" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">12</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09473" class="line-clamp-3 cursor-pointer text-balance">StyleSplat: 3D Object Style Transfer with Gaussian Splatting</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09473" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Prakanshul Saxena" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Prabhdeep Singh Sethi" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Avik Kuthiala" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Sahil Jain" style="content-visibility:auto;">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								4 authors
							</div></li></ul></a>
					<a href="/papers/2407.09473#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							3</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09413" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09413.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg" crossorigin="anonymous">
			davanstrien</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09413" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">11</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09413" class="line-clamp-3 cursor-pointer text-balance">SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09413" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Rama Chellappa" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="vsubhashini" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ccef359e4442f560a87fd66228e54c8b.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Shraman" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/88a6bcfeee99c12496940575a2fc65e9.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								3 authors
							</div></li></ul></a>
					<a href="/papers/2407.09413#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							3</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09072" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09072.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/b49d125d112a01106b48a2fcda481c3e.svg" crossorigin="anonymous">
			hetong007</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09072" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">11</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09072" class="line-clamp-3 cursor-pointer text-balance">New Desiderata for Direct Preference Optimization</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09072" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="David Wipf" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Xiangkun Hu" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="hetong007" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/b49d125d112a01106b48a2fcda481c3e.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								3 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.09072#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								4</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.08892" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08892.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/f4793a7367d372def34014551db48be2.svg" crossorigin="anonymous">
			sidjha1</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.08892" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">11</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.08892" class="line-clamp-3 cursor-pointer text-balance">Characterizing Prompt Compression Methods for Long Context Inference</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.08892" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Amir Gholami" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Kurt Keutzer" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Sehoon Kim" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Lutfi Eren Erdogan" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="sidjha1" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/f4793a7367d372def34014551db48be2.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								5 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.08892#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								2</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09012" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09012.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09012" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">10</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09012" class="line-clamp-3 cursor-pointer text-balance">TCAN: Animating Human Images with Temporally Consistent Pose Guidance
  using Diffusion Models</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09012" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jaegul Choo" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Junsoo Lee" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Min-Jung Kim" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="rlawjdghek" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/36a30c0f9e03488b24313be4c1a6ed59.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								4 authors
							</div></li></ul></a>
					<a href="/papers/2407.09012#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09732" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09732.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg" crossorigin="anonymous">
			xi-j</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09732" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">9</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09732" class="line-clamp-3 cursor-pointer text-balance">Speech Slytherin: Examining the Performance and Efficiency of Mamba for
  Speech Separation, Recognition, and Synthesis</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09732" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Nima Mesgarani" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Cong Han" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Adrian Nicolas Florea" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Yinghao Aaron Li" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="xi-j" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								5 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.09732#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								2</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2406.02265" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02265.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg" crossorigin="anonymous">
			akhaliq</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2406.02265" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">7</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2406.02265" class="line-clamp-3 cursor-pointer text-balance">Understanding Retrieval Robustness for Retrieval-Augmented Image
  Captioning</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2406.02265" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Desmond Elliott" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Raphael Tang" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Rita Ramos" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="jaagli" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/581f5d56b393b95c74f38a06f6bb4f69.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="lyan62" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								5 authors
							</div></li></ul></a>
					<a href="/papers/2406.02265#community" class="ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
							2</a></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.09121" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09121.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/a97c4970a1a179ee8a2e2e6ab8f995f6.svg" crossorigin="anonymous">
			Youliang</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.09121" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">6</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.09121" class="line-clamp-3 cursor-pointer text-balance">Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled
  Refusal Training</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.09121" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jen-tse Huang" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Wenxuan Wang" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Wenxiang Jiao" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="zptu" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/8a4cc63bd7be388010bc329bb74582a1.svg">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Youliang" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/a97c4970a1a179ee8a2e2e6ab8f995f6.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								8 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.09121#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								2</a></span>
	</span></div></div></div></div></article><article class="relative flex flex-col overflow-hidden rounded-xl border"><a href="/papers/2407.06397" class="shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06397.png" loading="lazy" decoding="async" alt="" class="h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert"></a>

	<div class="shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64">Submitted by
		<img alt="" loading="lazy" class="w-2.5 h-2.5 rounded-full  flex-none" src="/avatars/38a28103b15d93c7d9243bc5418e623e.svg" crossorigin="anonymous">
			Jovphi</div>

	

	<div class="from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8"><div class="flex w-full gap-6"><div class="flex flex-wrap items-center gap-2.5 pt-1  lg:sticky lg:top-8"><a href="/login?next=%2Fpapers%2F2407.06397" class="self-start">

<div class="shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850"><input disabled type="checkbox"  class="peer hidden">
		<svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg>
		<div class="leading-none">5</div></div></a>
	</div>




			<div class="w-full"><h3 class="mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6"><a href="/papers/2407.06397" class="line-clamp-3 cursor-pointer text-balance">RRM: Relightable assets using Radiance guided Material extraction</a></h3>
				<div class="flex items-center justify-between"><a href="/papers/2407.06397" class="flex"><ul class="flex items-center  flex-row-reverse   text-sm  "><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Élie Michel" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Adrien Kaiser" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Diego Gomez" style="content-visibility:auto;">
			</li><li class="  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="Jovphi" style="content-visibility:auto;"><img class="overflow-hidden rounded-full" alt="" src="/avatars/38a28103b15d93c7d9243bc5418e623e.svg">
			</li>

		<li class="text-gray-600 hover:text-gray-700 order-first ml-3"><div class="flex truncate text-base text-gray-350"><div class="ml-1 mr-2.5">·</div>
								4 authors
							</div></li></ul></a>
					

<span class="inline-block "><span class="contents"><a slot="anchor" href="/papers/2407.06397#community" class="ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path></svg>
								2</a></span>
	</span></div></div></div></div></article>

		

		<div class="col-span-1 mt-8 flex lg:col-span-2 xl:col-span-3"><a class="btn gap-2" href="/papers/date/2024-07-12"><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z" fill="currentColor"></path></svg>Previous
				</a>
			<a class="btn ml-auto gap-2" href="/papers/date/2024-07-16">Next<svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z" fill="currentColor"></path></svg></a></div></div>
</section></div></main>

	<footer class="b-12 mb-2 flex border-t border-gray-100 md:h-14"><nav class="container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm"><div class="SVELTE_HYDRATER contents" data-target="ThemeSwitcher" data-props="{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}">
<div class="relative inline-block max-md:mb-5 max-md:*:self-start">
	<button class="rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 " type="button">
		<svg class="mr-1.5 text-gray-500" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z" fill="currentColor"></path><path d="M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z" fill="currentColor" fill-opacity=".4"></path><path d="M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z" fill="currentColor"></path></svg>
			System theme
		</button>
	
	
	</div></div>
		<div class="font-semibold text-black md:hidden">Company</div>
		<a class="hover:underline" href="/terms-of-service">TOS</a>
		<a class="hover:underline" href="/privacy">Privacy</a>
		<a class="hover:underline" href="/huggingface">About</a>
		<a class="hover:underline" href="https://apply.workable.com/huggingface/">Jobs</a>
		<a href="/" class="max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last"><svg class="h-7 w-7 transition-transform group-hover:-translate-y-px" viewBox="0 0 95 88" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z" fill="#FFD21E"></path><path d="M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z" fill="#FF9D0B"></path><path d="M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z" fill="#3A3B45"></path><path d="M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z" fill="#3A3B45"></path><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="#3A3B45"></path><mask id="mask0" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="33" y="41" width="27" height="16"><path d="M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z" fill="white"></path></mask><g mask="url(#mask0)"><path d="M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z" fill="#F94040"></path></g><path d="M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z" fill="#FF9D0B"></path><path d="M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z" fill="#FF9D0B"></path><path class="origin-bottom-right transition-transform group-hover:-rotate-6" d="M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z" fill="#FFD21E"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z" fill="#FF9D0B"></path><path class="origin-bottom-left transition-transform group-hover:rotate-6" d="M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z" fill="#FFD21E"></path></svg></a>
		<div class="max-md:mt-8! font-semibold text-black md:hidden">Website</div>

		<a class="hover:underline" href="/models">Models</a>
		<a class="hover:underline" href="/datasets">Datasets</a>
		<a class="hover:underline" href="/spaces">Spaces</a>
		<a class="hover:underline" href="/pricing">Pricing</a>
		<a class="hover:underline" href="/docs">Docs</a></nav></footer></div>

		<script>
			import("\/front\/build\/kube-a3d300f\/index.js");
			window.moonSha = "kube-a3d300f\/";
			window.__hf_deferred = {};
		</script>

		<!-- Stripe -->
		<script>
			if (["hf.co", "huggingface.co"].includes(window.location.hostname)) {
				const script = document.createElement("script");
				script.src = "https://js.stripe.com/v3/";
				script.async = true;
				document.head.appendChild(script);
			}
		</script>
	</body>
</html>
